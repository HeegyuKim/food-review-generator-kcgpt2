{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cVVZl_t4WLoT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPT-2 학습을 위한 데이터 생성\n",
        "긍정적인 리뷰는 `긍정: 내용`으로 변경, 부정적인 리뷰는 `부정: 내용`으로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YgBvVsqXagft"
      },
      "outputs": [],
      "source": [
        "dir_path = \"data/\"\n",
        "input_file = dir_path + \"poi_review_corpus.txt\"\n",
        "train_src_file = dir_path + \"poi_review_gpt2_train.txt\"\n",
        "test_src_file = dir_path + \"poi_review_gpt2_test.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y9SzWCWsWP4L"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(input_file, sep=\"\\t\", encoding='utf-8')\n",
        "df.label = df.label.map({0: \"부정 : \", 1: \"긍정 : \"})\n",
        "text = df.label.str.cat(df.content)\n",
        "\n",
        "train_size = len(text) // 10 * 8\n",
        "\n",
        "def write_txt(filename, series):\n",
        "  with open(filename, \"w\", encoding='utf-8') as f:\n",
        "    f.write(\"\\n\".join(series.values))\n",
        "\n",
        "write_txt(train_src_file, text[:train_size])\n",
        "write_txt(test_src_file, text[train_size:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk9UGK_MXSn2",
        "outputId": "7ca37898-bb81-4dcf-9bd4-6a8f8335649d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'head'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n",
            "'head'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
            "��ġ ������ �ƴմϴ�.\n"
          ]
        }
      ],
      "source": [
        "!head -n 3 /content/drive/MyDrive/data/poi-review/poi_review_gpt2_train.txt\n",
        "!head -n 3 /content/drive/MyDrive/data/poi-review/poi_review_gpt2_test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWr-4IStYkS7",
        "outputId": "23a8f46a-83e7-4d98-d833-966fa403df6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 872/872 [00:00<00:00, 871kB/s]\n",
            "Downloading: 100%|██████████| 1.28M/1.28M [00:01<00:00, 823kB/s] \n",
            "Downloading: 100%|██████████| 948k/948k [00:01<00:00, 674kB/s] \n",
            "D:\\Anaconda3\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:907: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "Downloading: 100%|██████████| 501M/501M [00:45<00:00, 11.5MB/s] \n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "    AutoConfig, AutoTokenizer, TextDataset, DataCollatorForLanguageModeling,\n",
        "    PreTrainedTokenizer,\n",
        "    Trainer, TrainingArguments,AutoModelWithLMHead\n",
        ")\n",
        "from typing import Dict, List, Optional\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class LineByLineTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach soon.\n",
        "    Source from https://github.com/huggingface/transformers/blob/db7d6a80e82d66127b2a44b6e3382969fdc8b207/src/transformers/data/datasets/language_modeling.py#L115\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, block_size: int):\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
        "\n",
        "        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size, padding=\"max_length\")\n",
        "        self.examples = batch_encoding[\"input_ids\"]\n",
        "        self.examples = [{\"input_ids\": torch.tensor(e, dtype=torch.long)} for e in self.examples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.tensor]:\n",
        "        return self.examples[i]\n",
        "\n",
        "def load_dataset(train_path,test_path,tokenizer):\n",
        "    train_dataset = LineByLineTextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "     \n",
        "    test_dataset = LineByLineTextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=test_path,\n",
        "          block_size=128)   \n",
        "    \n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset,test_dataset,data_collator\n",
        "\n",
        "\n",
        "model_name = \"beomi/kcgpt2\"\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelWithLMHead.from_pretrained(model_name)\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "train_dataset,test_dataset,data_collator = load_dataset(train_src_file, test_src_file, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bmMF4zp9ZZbL"
      },
      "outputs": [],
      "source": [
        "model_path = \"checkpoint/\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_path, #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=2, # number of training epochs\n",
        "    per_device_train_batch_size=16, # batch size for training\n",
        "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=800, # after # steps model is saved \n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True,\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "jViZ5HGOZkCy",
        "outputId": "8c8671ef-408e-409c-ef01-1c27793dbdc4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "D:\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 80000\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 10000\n",
            "  5%|▌         | 500/10000 [05:28<1:41:43,  1.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.7492, 'learning_rate': 5e-05, 'epoch': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 800/10000 [08:40<1:37:52,  1.57it/s]Saving model checkpoint to checkpoint/checkpoint-800\n",
            "Configuration saved in checkpoint/checkpoint-800\\config.json\n",
            "Model weights saved in checkpoint/checkpoint-800\\pytorch_model.bin\n",
            " 10%|█         | 1000/10000 [10:51<1:37:10,  1.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.5092, 'learning_rate': 4.736842105263158e-05, 'epoch': 0.2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 1500/10000 [16:15<1:32:44,  1.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.4226, 'learning_rate': 4.473684210526316e-05, 'epoch': 0.3}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 1600/10000 [17:21<1:28:36,  1.58it/s]Saving model checkpoint to checkpoint/checkpoint-1600\n",
            "Configuration saved in checkpoint/checkpoint-1600\\config.json\n",
            "Model weights saved in checkpoint/checkpoint-1600\\pytorch_model.bin\n",
            " 20%|██        | 2000/10000 [21:45<1:31:52,  1.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.3754, 'learning_rate': 4.210526315789474e-05, 'epoch': 0.4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 2400/10000 [26:19<1:26:19,  1.47it/s]Saving model checkpoint to checkpoint/checkpoint-2400\n",
            "Configuration saved in checkpoint/checkpoint-2400\\config.json\n",
            "Model weights saved in checkpoint/checkpoint-2400\\pytorch_model.bin\n",
            " 25%|██▌       | 2500/10000 [27:30<1:25:48,  1.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.349, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 3000/10000 [33:09<1:14:17,  1.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.3147, 'learning_rate': 3.6842105263157895e-05, 'epoch': 0.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 3200/10000 [34:58<55:43,  2.03it/s]  Saving model checkpoint to checkpoint/checkpoint-3200\n",
            "Configuration saved in checkpoint/checkpoint-3200\\config.json\n",
            "Model weights saved in checkpoint/checkpoint-3200\\pytorch_model.bin\n",
            " 35%|███▌      | 3500/10000 [37:37<54:35,  1.98it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.2855, 'learning_rate': 3.421052631578947e-05, 'epoch': 0.7}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 4000/10000 [41:35<28:12,  3.54it/s]  Saving model checkpoint to checkpoint/checkpoint-4000\n",
            "Configuration saved in checkpoint/checkpoint-4000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.2606, 'learning_rate': 3.157894736842105e-05, 'epoch': 0.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in checkpoint/checkpoint-4000\\pytorch_model.bin\n",
            " 45%|████▌     | 4500/10000 [45:14<45:06,  2.03it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.24, 'learning_rate': 2.8947368421052634e-05, 'epoch': 0.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 4800/10000 [47:47<54:20,  1.59it/s]Saving model checkpoint to checkpoint/checkpoint-4800\n",
            "Configuration saved in checkpoint/checkpoint-4800\\config.json\n",
            "Model weights saved in checkpoint/checkpoint-4800\\pytorch_model.bin\n",
            " 50%|█████     | 5000/10000 [49:33<40:55,  2.04it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.2289, 'learning_rate': 2.6315789473684212e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 5500/10000 [53:47<37:33,  2.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.957, 'learning_rate': 2.368421052631579e-05, 'epoch': 1.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 5600/10000 [54:37<36:44,  2.00it/s]Saving model checkpoint to checkpoint/checkpoint-5600\n",
            "Configuration saved in checkpoint/checkpoint-5600\\config.json\n",
            "Model weights saved in checkpoint/checkpoint-5600\\pytorch_model.bin\n",
            " 60%|██████    | 6000/10000 [58:30<33:13,  2.01it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9345, 'learning_rate': 2.105263157894737e-05, 'epoch': 1.2}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 6400/10000 [1:01:49<29:45,  2.02it/s]Saving model checkpoint to checkpoint/checkpoint-6400\n",
            "Configuration saved in checkpoint/checkpoint-6400\\config.json\n",
            "Model weights saved in checkpoint/checkpoint-6400\\pytorch_model.bin\n",
            " 65%|██████▌   | 6500/10000 [1:02:41<28:54,  2.02it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9479, 'learning_rate': 1.8421052631578947e-05, 'epoch': 1.3}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7000/10000 [1:06:50<24:39,  2.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.941, 'learning_rate': 1.5789473684210526e-05, 'epoch': 1.4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 7200/10000 [1:08:30<23:00,  2.03it/s]Saving model checkpoint to checkpoint/checkpoint-7200\n",
            "Configuration saved in checkpoint/checkpoint-7200\\config.json\n",
            "Model weights saved in checkpoint/checkpoint-7200\\pytorch_model.bin\n",
            " 75%|███████▌  | 7500/10000 [1:10:59<20:26,  2.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9406, 'learning_rate': 1.3157894736842106e-05, 'epoch': 1.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 8000/10000 [1:15:05<16:20,  2.04it/s]Saving model checkpoint to checkpoint/checkpoint-8000\n",
            "Configuration saved in checkpoint/checkpoint-8000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9336, 'learning_rate': 1.0526315789473684e-05, 'epoch': 1.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in checkpoint/checkpoint-8000\\pytorch_model.bin\n",
            " 85%|████████▌ | 8500/10000 [1:19:13<12:15,  2.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9352, 'learning_rate': 7.894736842105263e-06, 'epoch': 1.7}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 8800/10000 [1:21:40<09:49,  2.04it/s]Saving model checkpoint to checkpoint/checkpoint-8800\n",
            "Configuration saved in checkpoint/checkpoint-8800\\config.json\n",
            "Model weights saved in checkpoint/checkpoint-8800\\pytorch_model.bin\n",
            " 90%|█████████ | 9000/10000 [1:23:26<08:52,  1.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9254, 'learning_rate': 5.263157894736842e-06, 'epoch': 1.8}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 9500/10000 [1:28:33<05:41,  1.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9254, 'learning_rate': 2.631578947368421e-06, 'epoch': 1.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 9600/10000 [1:29:41<04:34,  1.46it/s]Saving model checkpoint to checkpoint/checkpoint-9600\n",
            "Configuration saved in checkpoint/checkpoint-9600\\config.json\n",
            "Model weights saved in checkpoint/checkpoint-9600\\pytorch_model.bin\n",
            "100%|██████████| 10000/10000 [1:34:16<00:00,  1.45it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100%|██████████| 10000/10000 [1:34:16<00:00,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9158, 'learning_rate': 0.0, 'epoch': 2.0}\n",
            "{'train_runtime': 5657.0009, 'train_samples_per_second': 28.284, 'train_steps_per_second': 1.768, 'train_loss': 4.154575244140625, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10000, training_loss=4.154575244140625, metrics={'train_runtime': 5657.0009, 'train_samples_per_second': 28.284, 'train_steps_per_second': 1.768, 'train_loss': 4.154575244140625, 'epoch': 2.0})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ylKEGhK9ZkXB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to checkpoint/\n",
            "Configuration saved in checkpoint/config.json\n",
            "Model weights saved in checkpoint/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCa3P0-KZl55"
      },
      "source": [
        "# pipeline을 이용한 모델 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "j9vlKj4BZmI-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file checkpoint/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"checkpoint/\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 55000\n",
            "}\n",
            "\n",
            "loading configuration file checkpoint/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"checkpoint/\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 55000\n",
            "}\n",
            "\n",
            "loading weights file checkpoint/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at checkpoint/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/beomi/kcgpt2/resolve/main/config.json from cache at C:\\Users\\heegyukim/.cache\\huggingface\\transformers\\6d23b85cd80b100c1e93dd47cd63fc842920dda4b9bdfc4fd6d4f505c4e6c14a.c8d24433cc735a046e6a9b6443e51d5276f508163ff7a7c6cd812df282bc4381\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"beomi/kcgpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 55000\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/beomi/kcgpt2/resolve/main/vocab.json from cache at C:\\Users\\heegyukim/.cache\\huggingface\\transformers\\8ce9edc033f676f2f0b853c4320611f4e15c48871e16be30a0cd938e3f14ff3d.aeda914de955cf1cb295df853aa8dcce7c9d4ebdcc09b9ccdbd3819e6699b169\n",
            "loading file https://huggingface.co/beomi/kcgpt2/resolve/main/merges.txt from cache at C:\\Users\\heegyukim/.cache\\huggingface\\transformers\\5dd5f2d67fe06458966e8ef0d3daf2332ef977d09c159d8cf5ae71378cee1faf.393700127647d371ee8dbd4358dd10b955f6561b7518c030b5dd3577f8decbf3\n",
            "loading file https://huggingface.co/beomi/kcgpt2/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/beomi/kcgpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/beomi/kcgpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/beomi/kcgpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/beomi/kcgpt2/resolve/main/config.json from cache at C:\\Users\\heegyukim/.cache\\huggingface\\transformers\\6d23b85cd80b100c1e93dd47cd63fc842920dda4b9bdfc4fd6d4f505c4e6c14a.c8d24433cc735a046e6a9b6443e51d5276f508163ff7a7c6cd812df282bc4381\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"beomi/kcgpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 55000\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/beomi/kcgpt2/resolve/main/config.json from cache at C:\\Users\\heegyukim/.cache\\huggingface\\transformers\\6d23b85cd80b100c1e93dd47cd63fc842920dda4b9bdfc4fd6d4f505c4e6c14a.c8d24433cc735a046e6a9b6443e51d5276f508163ff7a7c6cd812df282bc4381\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"beomi/kcgpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 55000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "reviewer = pipeline('text-generation',model=model_path, tokenizer=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aA9DJZntZ0x0"
      },
      "outputs": [],
      "source": [
        "def generate_review(reviewer, prefix, positive):\n",
        "  text = \"긍정 : \" if positive else \"부정 : \"\n",
        "  text += prefix\n",
        "  return reviewer(text, num_return_sequences =5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3wvmIwy-aBQ7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': '긍정 : 가성비 좋아요. 커피도  맛나구요.  맛있고 좋아요. 사장님들이 다 친절해요.  커피맛이  좋네요. 또 재방문하겠습니다. 사장님 감사합니다.  화이팅~~~!!  ☎☎�'},\n",
              " {'generated_text': '긍정 : 가성비 좋아요. 커피도 (스콘 맛있어요!) 마싯네요^^* 번창하세요 강추합니다 강추합니다 사장님 번창하세요~!!!♡♡♡♡♡ 강추해요~^^* 또'},\n",
              " {'generated_text': '긍정 : 가성비 좋아요. 커피도  굿!!  특히 아메리카노는 완전 맛있어요 ^^:)))))))))))))))))))  커피도 진짜 맛있고  맛도 미쳤어요 ㅎ'},\n",
              " {'generated_text': '긍정 : 가성비 좋아요. 커피도  맛있고  분위기도  최고 입니다~^^  사장님 오래오래 근무해주시길!!~^^  강추~!^^ (1/10~)/  (2/10~'},\n",
              " {'generated_text': '긍정 : 가성비 좋아요. 커피도  짱맛!~~*^^*♡**&amp;&amp;♡* 사장님 넘 친절하시네욤~~~~*^^*^*♡*&amp;♡.*♡*%'}]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_review(reviewer, \"가성비 좋아요. 커피도 \", True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xzhm4FYAaUat"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': '부정 : 바깥보다 못한 실내에 이마를 탁 치고 갑니다 손님에 대한  반말은 기본 ;; 직원들 응대부터 고치시길  진짜 이렇게 일처리 하면서 일하면 잘되나요? 아무리 코로나여도 다시는 안감..  직원'},\n",
              " {'generated_text': '부정 : 바깥보다 못한 실내에 이마를 탁 치고 갑니다 손님에 대한  배려가 없어서 불편합니다.  아 참고로 제가 점장인데요 제가 점주이신지는 모르겠지만 아무튼 제가 남자라서 여깁니다.  저는 남자라서 점장'},\n",
              " {'generated_text': '부정 : 바깥보다 못한 실내에 이마를 탁 치고 갑니다 손님에 대한  배려가 없는 가게입니다 다신 안갑니다  1층 직원도 불친절하고  2층 직원이나 여자직원들도 불친절하고 3층 직원이나 여자직원'},\n",
              " {'generated_text': '부정 : 바깥보다 못한 실내에 이마를 탁 치고 갑니다 손님에 대한  신경은 1도 안씀. 사장님들 너무 불친절  주문해도 먹고 안갈수도..  매장 이용중인데  여긴 안갈래요. 직원분'},\n",
              " {'generated_text': '부정 : 바깥보다 못한 실내에 이마를 탁 치고 갑니다 손님에 대한  예의는 1도 없고, 음식에 나오는  잡내나는 음식도 서비스로 제공되지않는 최악의 환경입니다 맛은  보통인데.. 비린내 너무 심합니다'}]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_review(reviewer, \"바깥보다 못한 실내에 이마를 탁 치고 갑니다 손님에 대한 \", False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8LQQJtdMn2k"
      },
      "source": [
        "# 디스크에서 불러오기\n",
        "https://huggingface.co/blog/how-to-generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fM1Hrvy_4G6J"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/beomi/kcgpt2/resolve/main/config.json from cache at C:\\Users\\heegyukim/.cache\\huggingface\\transformers\\6d23b85cd80b100c1e93dd47cd63fc842920dda4b9bdfc4fd6d4f505c4e6c14a.c8d24433cc735a046e6a9b6443e51d5276f508163ff7a7c6cd812df282bc4381\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"beomi/kcgpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 55000\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/beomi/kcgpt2/resolve/main/vocab.json from cache at C:\\Users\\heegyukim/.cache\\huggingface\\transformers\\8ce9edc033f676f2f0b853c4320611f4e15c48871e16be30a0cd938e3f14ff3d.aeda914de955cf1cb295df853aa8dcce7c9d4ebdcc09b9ccdbd3819e6699b169\n",
            "loading file https://huggingface.co/beomi/kcgpt2/resolve/main/merges.txt from cache at C:\\Users\\heegyukim/.cache\\huggingface\\transformers\\5dd5f2d67fe06458966e8ef0d3daf2332ef977d09c159d8cf5ae71378cee1faf.393700127647d371ee8dbd4358dd10b955f6561b7518c030b5dd3577f8decbf3\n",
            "loading file https://huggingface.co/beomi/kcgpt2/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/beomi/kcgpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/beomi/kcgpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/beomi/kcgpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/beomi/kcgpt2/resolve/main/config.json from cache at C:\\Users\\heegyukim/.cache\\huggingface\\transformers\\6d23b85cd80b100c1e93dd47cd63fc842920dda4b9bdfc4fd6d4f505c4e6c14a.c8d24433cc735a046e6a9b6443e51d5276f508163ff7a7c6cd812df282bc4381\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"beomi/kcgpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 55000\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/beomi/kcgpt2/resolve/main/config.json from cache at C:\\Users\\heegyukim/.cache\\huggingface\\transformers\\6d23b85cd80b100c1e93dd47cd63fc842920dda4b9bdfc4fd6d4f505c4e6c14a.c8d24433cc735a046e6a9b6443e51d5276f508163ff7a7c6cd812df282bc4381\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"beomi/kcgpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 55000\n",
            "}\n",
            "\n",
            "D:\\Anaconda3\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:907: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "loading configuration file trained_models/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"trained_models/\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 0,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 55000\n",
            "}\n",
            "\n",
            "loading weights file trained_models/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at trained_models/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "trained_path = \"trained_models/\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelWithLMHead.from_pretrained(trained_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mV4Cp_pNusW",
        "outputId": "4898a1c2-bd7a-4bbd-ca13-4d71322ce418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "긍정: 가성비 좋아요. 커피도  맛있어요. 직원분도 친절하셔서  좋네요. 번창하세요~^^^^^^^^^^^^^^^^^^^^^^^^^\n"
          ]
        }
      ],
      "source": [
        "input_text = \"긍정: 가성비 좋아요. 커피도 \"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids, max_length=50,\n",
        "    num_beams=10, \n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=2.0,\n",
        "    top_p=0.92,\n",
        "    early_stopping=True)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "L2SgbC86Ogdt"
      },
      "outputs": [],
      "source": [
        "# model quantization using\n",
        "# https://pytorch.org/docs/stable/quantization.html\n",
        "# https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html\n",
        "\n",
        "from transformers.modeling_utils import Conv1D\n",
        "from inspect import getmembers\n",
        "\n",
        "q_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear, Conv1D}, dtype=torch.qint8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9AQHUVT2Q1L1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "긍정: 가성비 좋아요. 커피도  맛있구요. 사장님도  친절하십니다.^^ 번창하세요~~~^^ 번창하세요~~~~^^^^^^^^^^^^^^^^\n"
          ]
        }
      ],
      "source": [
        "input_text = \"긍정: 가성비 좋아요. 커피도 \"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "outputs = q_model.generate(input_ids, max_length=50,\n",
        "    num_beams=10, \n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    temperature=2.0,\n",
        "    top_p=0.92,\n",
        "    early_stopping=True)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "gpt2-generation-poi-review.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
